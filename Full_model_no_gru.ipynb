{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw1RiUybIUY-",
        "outputId": "b066dee8-cf8a-48ed-9615-a8496a8d0ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YooN5r0IKo8"
      },
      "outputs": [],
      "source": [
        "gestures = [\n",
        "    'Above ear - pull hair',\n",
        "    'Cheek - pinch skin',\n",
        "    'Drink from bottle/cup',\n",
        "    'Eyebrow - pull hair',\n",
        "    'Eyelash - pull hair',\n",
        "    'Feel around in tray and pull out an object',\n",
        "    'Forehead - pull hairline',\n",
        "    'Forehead - scratch',\n",
        "    'Glasses on/off',\n",
        "    'Neck - pinch skin',\n",
        "    'Neck - scratch',\n",
        "    'Pinch knee/leg skin',\n",
        "    'Pull air toward your face',\n",
        "    'Scratch knee/leg skin',\n",
        "    'Text on phone',\n",
        "    'Wave hello',\n",
        "    'Write name in air',\n",
        "    'Write name on leg'\n",
        "]\n",
        "\n",
        "gesture_dict = {i: g for i, g in enumerate(gestures)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZJ-pf1H8Itp",
        "outputId": "404cc68d-e821-4644-fa41-2685dfeb5a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying IMU feature engineering...\n",
            "IMU features expanded to 20 dimensions\n",
            "\n",
            "=== Fold 1/5 ===\n",
            "Training samples: 19869\n",
            "Validation samples: 1528\n",
            "E1: Loss=1.7340 | F1-bin=0.9692 | F1-mac=0.5894 | S=0.7793\n",
            "Saved new best model with score: 0.7793\n",
            "E2: Loss=1.1764 | F1-bin=0.9774 | F1-mac=0.6682 | S=0.8228\n",
            "Saved new best model with score: 0.8228\n",
            "E3: Loss=0.9584 | F1-bin=0.9881 | F1-mac=0.7044 | S=0.8462\n",
            "Saved new best model with score: 0.8462\n",
            "E4: Loss=0.8249 | F1-bin=0.9830 | F1-mac=0.7106 | S=0.8468\n",
            "Saved new best model with score: 0.8468\n",
            "E5: Loss=0.7318 | F1-bin=0.9922 | F1-mac=0.7273 | S=0.8597\n",
            "Saved new best model with score: 0.8597\n",
            "E6: Loss=0.6462 | F1-bin=0.9870 | F1-mac=0.7198 | S=0.8534\n",
            "E7: Loss=0.6148 | F1-bin=0.9932 | F1-mac=0.7146 | S=0.8539\n",
            "E8: Loss=0.4910 | F1-bin=0.9922 | F1-mac=0.7484 | S=0.8703\n",
            "Saved new best model with score: 0.8703\n",
            "E9: Loss=0.4778 | F1-bin=0.9860 | F1-mac=0.7209 | S=0.8534\n",
            "E10: Loss=0.4290 | F1-bin=0.9927 | F1-mac=0.7379 | S=0.8653\n",
            "E11: Loss=0.4351 | F1-bin=0.9917 | F1-mac=0.7558 | S=0.8737\n",
            "Saved new best model with score: 0.8737\n",
            "E12: Loss=0.4439 | F1-bin=0.9943 | F1-mac=0.7724 | S=0.8833\n",
            "Saved new best model with score: 0.8833\n",
            "E13: Loss=0.3583 | F1-bin=0.9871 | F1-mac=0.7635 | S=0.8753\n",
            "E14: Loss=0.3425 | F1-bin=0.9937 | F1-mac=0.7620 | S=0.8779\n",
            "E15: Loss=0.3637 | F1-bin=0.9922 | F1-mac=0.7796 | S=0.8859\n",
            "Saved new best model with score: 0.8859\n",
            "E16: Loss=0.3498 | F1-bin=0.9943 | F1-mac=0.7658 | S=0.8801\n",
            "E17: Loss=0.3139 | F1-bin=0.9906 | F1-mac=0.7752 | S=0.8829\n",
            "E18: Loss=0.3272 | F1-bin=0.9922 | F1-mac=0.7899 | S=0.8910\n",
            "Saved new best model with score: 0.8910\n",
            "E19: Loss=0.3015 | F1-bin=0.9922 | F1-mac=0.7620 | S=0.8771\n",
            "E20: Loss=0.2789 | F1-bin=0.9922 | F1-mac=0.7803 | S=0.8863\n",
            "E21: Loss=0.2945 | F1-bin=0.9881 | F1-mac=0.7760 | S=0.8820\n",
            "E22: Loss=0.2928 | F1-bin=0.9937 | F1-mac=0.7780 | S=0.8858\n",
            "E23: Loss=0.3083 | F1-bin=0.9932 | F1-mac=0.7732 | S=0.8832\n",
            "E24: Loss=0.3143 | F1-bin=0.9938 | F1-mac=0.7536 | S=0.8737\n",
            "E25: Loss=0.2501 | F1-bin=0.9938 | F1-mac=0.7754 | S=0.8846\n",
            "E26: Loss=0.2755 | F1-bin=0.9927 | F1-mac=0.7847 | S=0.8887\n",
            "E27: Loss=0.2790 | F1-bin=0.9958 | F1-mac=0.7549 | S=0.8754\n",
            "E28: Loss=0.2547 | F1-bin=0.9922 | F1-mac=0.7576 | S=0.8749\n",
            "E29: Loss=0.2597 | F1-bin=0.9912 | F1-mac=0.7811 | S=0.8862\n",
            "E30: Loss=0.2329 | F1-bin=0.9897 | F1-mac=0.7533 | S=0.8715\n",
            "E31: Loss=0.2429 | F1-bin=0.9917 | F1-mac=0.7664 | S=0.8790\n",
            "E32: Loss=0.2141 | F1-bin=0.9938 | F1-mac=0.7700 | S=0.8819\n",
            "E33: Loss=0.2192 | F1-bin=0.9850 | F1-mac=0.7685 | S=0.8768\n",
            "E34: Loss=0.2380 | F1-bin=0.9938 | F1-mac=0.7922 | S=0.8930\n",
            "Saved new best model with score: 0.8930\n",
            "E35: Loss=0.2226 | F1-bin=0.9922 | F1-mac=0.7984 | S=0.8953\n",
            "Saved new best model with score: 0.8953\n",
            "E36: Loss=0.2418 | F1-bin=0.9886 | F1-mac=0.7933 | S=0.8910\n",
            "E37: Loss=0.2400 | F1-bin=0.9943 | F1-mac=0.7533 | S=0.8738\n",
            "E38: Loss=0.2540 | F1-bin=0.9871 | F1-mac=0.7842 | S=0.8857\n",
            "E39: Loss=0.2158 | F1-bin=0.9938 | F1-mac=0.7877 | S=0.8908\n",
            "E40: Loss=0.2189 | F1-bin=0.9943 | F1-mac=0.7804 | S=0.8873\n",
            "E41: Loss=0.2244 | F1-bin=0.9938 | F1-mac=0.7862 | S=0.8900\n",
            "E42: Loss=0.2183 | F1-bin=0.9907 | F1-mac=0.7909 | S=0.8908\n",
            "E43: Loss=0.2001 | F1-bin=0.9938 | F1-mac=0.7812 | S=0.8875\n",
            "E44: Loss=0.1951 | F1-bin=0.9953 | F1-mac=0.7904 | S=0.8928\n",
            "E45: Loss=0.1862 | F1-bin=0.9896 | F1-mac=0.7682 | S=0.8789\n",
            "E46: Loss=0.2081 | F1-bin=0.9927 | F1-mac=0.7778 | S=0.8852\n",
            "E47: Loss=0.2029 | F1-bin=0.9932 | F1-mac=0.7929 | S=0.8931\n",
            "E48: Loss=0.1931 | F1-bin=0.9907 | F1-mac=0.7945 | S=0.8926\n",
            "E49: Loss=0.2096 | F1-bin=0.9964 | F1-mac=0.7853 | S=0.8908\n",
            "E50: Loss=0.2198 | F1-bin=0.9922 | F1-mac=0.7890 | S=0.8906\n",
            "Best score fold 1: 0.8953\n",
            "\n",
            "=== Fold 2/5 ===\n",
            "Training samples: 19557\n",
            "Validation samples: 1632\n",
            "E1: Loss=1.6582 | F1-bin=0.9315 | F1-mac=0.5156 | S=0.7235\n",
            "Saved new best model with score: 0.7235\n",
            "E2: Loss=1.1745 | F1-bin=0.9611 | F1-mac=0.6071 | S=0.7841\n",
            "Saved new best model with score: 0.7841\n",
            "E3: Loss=0.9940 | F1-bin=0.9655 | F1-mac=0.6234 | S=0.7944\n",
            "Saved new best model with score: 0.7944\n",
            "E4: Loss=0.8085 | F1-bin=0.9769 | F1-mac=0.6457 | S=0.8113\n",
            "Saved new best model with score: 0.8113\n",
            "E5: Loss=0.7096 | F1-bin=0.9786 | F1-mac=0.6873 | S=0.8330\n",
            "Saved new best model with score: 0.8330\n",
            "E6: Loss=0.6294 | F1-bin=0.9815 | F1-mac=0.6807 | S=0.8311\n",
            "E7: Loss=0.5774 | F1-bin=0.9826 | F1-mac=0.6705 | S=0.8266\n",
            "E8: Loss=0.5034 | F1-bin=0.9798 | F1-mac=0.6783 | S=0.8291\n",
            "E9: Loss=0.4616 | F1-bin=0.9831 | F1-mac=0.6822 | S=0.8326\n",
            "E10: Loss=0.4454 | F1-bin=0.9776 | F1-mac=0.6926 | S=0.8351\n",
            "Saved new best model with score: 0.8351\n",
            "E11: Loss=0.4067 | F1-bin=0.9780 | F1-mac=0.6908 | S=0.8344\n",
            "E12: Loss=0.4025 | F1-bin=0.9835 | F1-mac=0.7046 | S=0.8441\n",
            "Saved new best model with score: 0.8441\n",
            "E13: Loss=0.4159 | F1-bin=0.9796 | F1-mac=0.6820 | S=0.8308\n",
            "E14: Loss=0.3510 | F1-bin=0.9812 | F1-mac=0.6653 | S=0.8232\n",
            "E15: Loss=0.3531 | F1-bin=0.9811 | F1-mac=0.6927 | S=0.8369\n",
            "E16: Loss=0.3264 | F1-bin=0.9819 | F1-mac=0.7008 | S=0.8414\n",
            "E17: Loss=0.3224 | F1-bin=0.9814 | F1-mac=0.6944 | S=0.8379\n",
            "E18: Loss=0.3053 | F1-bin=0.9820 | F1-mac=0.7191 | S=0.8506\n",
            "Saved new best model with score: 0.8506\n",
            "E19: Loss=0.2889 | F1-bin=0.9854 | F1-mac=0.7196 | S=0.8525\n",
            "Saved new best model with score: 0.8525\n",
            "E20: Loss=0.3177 | F1-bin=0.9822 | F1-mac=0.7037 | S=0.8429\n",
            "E21: Loss=0.2896 | F1-bin=0.9794 | F1-mac=0.6906 | S=0.8350\n",
            "E22: Loss=0.2967 | F1-bin=0.9864 | F1-mac=0.7068 | S=0.8466\n",
            "E23: Loss=0.2751 | F1-bin=0.9814 | F1-mac=0.7012 | S=0.8413\n",
            "E24: Loss=0.2732 | F1-bin=0.9788 | F1-mac=0.7185 | S=0.8487\n",
            "E25: Loss=0.2719 | F1-bin=0.9736 | F1-mac=0.6960 | S=0.8348\n",
            "E26: Loss=0.2633 | F1-bin=0.9834 | F1-mac=0.7094 | S=0.8464\n",
            "E27: Loss=0.2572 | F1-bin=0.9819 | F1-mac=0.7012 | S=0.8415\n",
            "E28: Loss=0.2702 | F1-bin=0.9839 | F1-mac=0.6959 | S=0.8399\n",
            "E29: Loss=0.2566 | F1-bin=0.9834 | F1-mac=0.7061 | S=0.8447\n",
            "E30: Loss=0.2471 | F1-bin=0.9806 | F1-mac=0.7142 | S=0.8474\n",
            "E31: Loss=0.2591 | F1-bin=0.9776 | F1-mac=0.7219 | S=0.8498\n",
            "E32: Loss=0.2055 | F1-bin=0.9798 | F1-mac=0.7053 | S=0.8426\n",
            "E33: Loss=0.2598 | F1-bin=0.9810 | F1-mac=0.7057 | S=0.8433\n",
            "E34: Loss=0.2265 | F1-bin=0.9844 | F1-mac=0.7072 | S=0.8458\n",
            "E35: Loss=0.2369 | F1-bin=0.9795 | F1-mac=0.7069 | S=0.8432\n",
            "E36: Loss=0.2317 | F1-bin=0.9813 | F1-mac=0.7093 | S=0.8453\n",
            "E37: Loss=0.2202 | F1-bin=0.9830 | F1-mac=0.7228 | S=0.8529\n",
            "Saved new best model with score: 0.8529\n",
            "E38: Loss=0.2018 | F1-bin=0.9824 | F1-mac=0.7150 | S=0.8487\n",
            "E39: Loss=0.2026 | F1-bin=0.9874 | F1-mac=0.7111 | S=0.8492\n",
            "E40: Loss=0.1907 | F1-bin=0.9809 | F1-mac=0.6961 | S=0.8385\n",
            "E41: Loss=0.2054 | F1-bin=0.9864 | F1-mac=0.6763 | S=0.8313\n",
            "E42: Loss=0.1944 | F1-bin=0.9834 | F1-mac=0.7128 | S=0.8481\n",
            "E43: Loss=0.2137 | F1-bin=0.9824 | F1-mac=0.7118 | S=0.8471\n",
            "E44: Loss=0.1953 | F1-bin=0.9785 | F1-mac=0.7158 | S=0.8471\n",
            "E45: Loss=0.1797 | F1-bin=0.9845 | F1-mac=0.7227 | S=0.8536\n",
            "Saved new best model with score: 0.8536\n",
            "E46: Loss=0.2096 | F1-bin=0.9782 | F1-mac=0.7113 | S=0.8447\n",
            "E47: Loss=0.2022 | F1-bin=0.9811 | F1-mac=0.7201 | S=0.8506\n",
            "E48: Loss=0.1832 | F1-bin=0.9740 | F1-mac=0.7092 | S=0.8416\n",
            "E49: Loss=0.1913 | F1-bin=0.9882 | F1-mac=0.7200 | S=0.8541\n",
            "Saved new best model with score: 0.8541\n",
            "E50: Loss=0.1876 | F1-bin=0.9809 | F1-mac=0.7145 | S=0.8477\n",
            "Best score fold 2: 0.8541\n",
            "\n",
            "=== Fold 3/5 ===\n",
            "Training samples: 19578\n",
            "Validation samples: 1625\n",
            "E1: Loss=1.6612 | F1-bin=0.9128 | F1-mac=0.5209 | S=0.7169\n",
            "Saved new best model with score: 0.7169\n",
            "E2: Loss=1.1252 | F1-bin=0.9529 | F1-mac=0.5849 | S=0.7689\n",
            "Saved new best model with score: 0.7689\n",
            "E3: Loss=0.9377 | F1-bin=0.9553 | F1-mac=0.6122 | S=0.7837\n",
            "Saved new best model with score: 0.7837\n",
            "E4: Loss=0.7741 | F1-bin=0.9531 | F1-mac=0.6730 | S=0.8130\n",
            "Saved new best model with score: 0.8130\n",
            "E5: Loss=0.6738 | F1-bin=0.9521 | F1-mac=0.6382 | S=0.7951\n",
            "E6: Loss=0.6226 | F1-bin=0.9574 | F1-mac=0.6665 | S=0.8120\n",
            "E7: Loss=0.5378 | F1-bin=0.9627 | F1-mac=0.6779 | S=0.8203\n",
            "Saved new best model with score: 0.8203\n",
            "E8: Loss=0.4955 | F1-bin=0.9688 | F1-mac=0.6971 | S=0.8330\n",
            "Saved new best model with score: 0.8330\n",
            "E9: Loss=0.4812 | F1-bin=0.9676 | F1-mac=0.6795 | S=0.8235\n",
            "E10: Loss=0.4288 | F1-bin=0.9654 | F1-mac=0.7156 | S=0.8405\n",
            "Saved new best model with score: 0.8405\n",
            "E11: Loss=0.3872 | F1-bin=0.9624 | F1-mac=0.6744 | S=0.8184\n",
            "E12: Loss=0.3954 | F1-bin=0.9690 | F1-mac=0.6972 | S=0.8331\n",
            "E13: Loss=0.3990 | F1-bin=0.9677 | F1-mac=0.6935 | S=0.8306\n",
            "E14: Loss=0.3659 | F1-bin=0.9683 | F1-mac=0.7040 | S=0.8362\n",
            "E15: Loss=0.3467 | F1-bin=0.9619 | F1-mac=0.6975 | S=0.8297\n",
            "E16: Loss=0.3177 | F1-bin=0.9652 | F1-mac=0.7146 | S=0.8399\n",
            "E17: Loss=0.3134 | F1-bin=0.9672 | F1-mac=0.7085 | S=0.8378\n",
            "E18: Loss=0.2758 | F1-bin=0.9592 | F1-mac=0.6920 | S=0.8256\n",
            "E19: Loss=0.3126 | F1-bin=0.9733 | F1-mac=0.7045 | S=0.8389\n",
            "E20: Loss=0.2929 | F1-bin=0.9599 | F1-mac=0.6928 | S=0.8264\n",
            "E21: Loss=0.2661 | F1-bin=0.9687 | F1-mac=0.6950 | S=0.8319\n",
            "E22: Loss=0.2836 | F1-bin=0.9666 | F1-mac=0.6985 | S=0.8325\n",
            "E23: Loss=0.2451 | F1-bin=0.9706 | F1-mac=0.7132 | S=0.8419\n",
            "Saved new best model with score: 0.8419\n",
            "E24: Loss=0.2567 | F1-bin=0.9728 | F1-mac=0.7112 | S=0.8420\n",
            "Saved new best model with score: 0.8420\n",
            "E25: Loss=0.2560 | F1-bin=0.9626 | F1-mac=0.6994 | S=0.8310\n",
            "E26: Loss=0.2514 | F1-bin=0.9607 | F1-mac=0.6917 | S=0.8262\n",
            "E27: Loss=0.2424 | F1-bin=0.9643 | F1-mac=0.7163 | S=0.8403\n",
            "E28: Loss=0.2393 | F1-bin=0.9623 | F1-mac=0.7128 | S=0.8376\n",
            "E29: Loss=0.2458 | F1-bin=0.9731 | F1-mac=0.7118 | S=0.8424\n",
            "Saved new best model with score: 0.8424\n",
            "E30: Loss=0.2509 | F1-bin=0.9708 | F1-mac=0.7354 | S=0.8531\n",
            "Saved new best model with score: 0.8531\n",
            "E31: Loss=0.2293 | F1-bin=0.9712 | F1-mac=0.7249 | S=0.8481\n",
            "E32: Loss=0.2164 | F1-bin=0.9682 | F1-mac=0.7200 | S=0.8441\n",
            "E33: Loss=0.2195 | F1-bin=0.9642 | F1-mac=0.6985 | S=0.8313\n",
            "E34: Loss=0.2226 | F1-bin=0.9662 | F1-mac=0.6848 | S=0.8255\n",
            "E35: Loss=0.2279 | F1-bin=0.9678 | F1-mac=0.7217 | S=0.8448\n",
            "E36: Loss=0.2091 | F1-bin=0.9669 | F1-mac=0.7203 | S=0.8436\n",
            "E37: Loss=0.2340 | F1-bin=0.9752 | F1-mac=0.7387 | S=0.8569\n",
            "Saved new best model with score: 0.8569\n",
            "E38: Loss=0.2068 | F1-bin=0.9737 | F1-mac=0.7094 | S=0.8416\n",
            "E39: Loss=0.1968 | F1-bin=0.9717 | F1-mac=0.7229 | S=0.8473\n",
            "E40: Loss=0.2078 | F1-bin=0.9750 | F1-mac=0.7240 | S=0.8495\n",
            "E41: Loss=0.1902 | F1-bin=0.9704 | F1-mac=0.7273 | S=0.8489\n",
            "E42: Loss=0.2309 | F1-bin=0.9686 | F1-mac=0.7317 | S=0.8501\n",
            "E43: Loss=0.1999 | F1-bin=0.9623 | F1-mac=0.7237 | S=0.8430\n",
            "E44: Loss=0.1749 | F1-bin=0.9692 | F1-mac=0.7106 | S=0.8399\n",
            "E45: Loss=0.1913 | F1-bin=0.9577 | F1-mac=0.7047 | S=0.8312\n",
            "E46: Loss=0.2016 | F1-bin=0.9621 | F1-mac=0.7077 | S=0.8349\n",
            "E47: Loss=0.1760 | F1-bin=0.9583 | F1-mac=0.6959 | S=0.8271\n",
            "E48: Loss=0.2019 | F1-bin=0.9643 | F1-mac=0.7068 | S=0.8355\n",
            "E49: Loss=0.1762 | F1-bin=0.9725 | F1-mac=0.7097 | S=0.8411\n",
            "E50: Loss=0.1763 | F1-bin=0.9742 | F1-mac=0.7228 | S=0.8485\n",
            "Best score fold 3: 0.8569\n",
            "\n",
            "=== Fold 4/5 ===\n",
            "Training samples: 19557\n",
            "Validation samples: 1632\n",
            "E1: Loss=1.6362 | F1-bin=0.9059 | F1-mac=0.4667 | S=0.6863\n",
            "Saved new best model with score: 0.6863\n",
            "E2: Loss=1.1223 | F1-bin=0.9250 | F1-mac=0.5357 | S=0.7304\n",
            "Saved new best model with score: 0.7304\n",
            "E3: Loss=0.9335 | F1-bin=0.9395 | F1-mac=0.5492 | S=0.7443\n",
            "Saved new best model with score: 0.7443\n",
            "E4: Loss=0.7852 | F1-bin=0.9484 | F1-mac=0.6129 | S=0.7807\n",
            "Saved new best model with score: 0.7807\n",
            "E5: Loss=0.6527 | F1-bin=0.9526 | F1-mac=0.6215 | S=0.7870\n",
            "Saved new best model with score: 0.7870\n",
            "E6: Loss=0.5832 | F1-bin=0.9557 | F1-mac=0.6312 | S=0.7934\n",
            "Saved new best model with score: 0.7934\n",
            "E7: Loss=0.5212 | F1-bin=0.9657 | F1-mac=0.6698 | S=0.8177\n",
            "Saved new best model with score: 0.8177\n",
            "E8: Loss=0.4865 | F1-bin=0.9639 | F1-mac=0.6491 | S=0.8065\n",
            "E9: Loss=0.4326 | F1-bin=0.9657 | F1-mac=0.6545 | S=0.8101\n",
            "E10: Loss=0.4280 | F1-bin=0.9629 | F1-mac=0.6407 | S=0.8018\n",
            "E11: Loss=0.4026 | F1-bin=0.9610 | F1-mac=0.6658 | S=0.8134\n",
            "E12: Loss=0.3743 | F1-bin=0.9558 | F1-mac=0.6370 | S=0.7964\n",
            "E13: Loss=0.3724 | F1-bin=0.9776 | F1-mac=0.6680 | S=0.8228\n",
            "Saved new best model with score: 0.8228\n",
            "E14: Loss=0.3307 | F1-bin=0.9651 | F1-mac=0.6483 | S=0.8067\n",
            "E15: Loss=0.3515 | F1-bin=0.9596 | F1-mac=0.6499 | S=0.8048\n",
            "E16: Loss=0.3290 | F1-bin=0.9576 | F1-mac=0.6647 | S=0.8111\n",
            "E17: Loss=0.3438 | F1-bin=0.9652 | F1-mac=0.6890 | S=0.8271\n",
            "Saved new best model with score: 0.8271\n",
            "E18: Loss=0.2814 | F1-bin=0.9665 | F1-mac=0.6723 | S=0.8194\n",
            "E19: Loss=0.2953 | F1-bin=0.9573 | F1-mac=0.6722 | S=0.8148\n",
            "E20: Loss=0.2708 | F1-bin=0.9657 | F1-mac=0.6732 | S=0.8195\n",
            "E21: Loss=0.2674 | F1-bin=0.9658 | F1-mac=0.6972 | S=0.8315\n",
            "Saved new best model with score: 0.8315\n",
            "E22: Loss=0.2738 | F1-bin=0.9686 | F1-mac=0.6622 | S=0.8154\n",
            "E23: Loss=0.2781 | F1-bin=0.9456 | F1-mac=0.6395 | S=0.7926\n",
            "E24: Loss=0.2380 | F1-bin=0.9673 | F1-mac=0.6827 | S=0.8250\n",
            "E25: Loss=0.2790 | F1-bin=0.9690 | F1-mac=0.6912 | S=0.8301\n",
            "E26: Loss=0.2493 | F1-bin=0.9628 | F1-mac=0.6752 | S=0.8190\n",
            "E27: Loss=0.2570 | F1-bin=0.9697 | F1-mac=0.6866 | S=0.8282\n",
            "E28: Loss=0.2296 | F1-bin=0.9601 | F1-mac=0.6816 | S=0.8209\n",
            "E29: Loss=0.2383 | F1-bin=0.9648 | F1-mac=0.6872 | S=0.8260\n",
            "E30: Loss=0.2083 | F1-bin=0.9660 | F1-mac=0.6669 | S=0.8164\n",
            "E31: Loss=0.2582 | F1-bin=0.9650 | F1-mac=0.6878 | S=0.8264\n",
            "E32: Loss=0.2192 | F1-bin=0.9655 | F1-mac=0.6575 | S=0.8115\n",
            "E33: Loss=0.1958 | F1-bin=0.9589 | F1-mac=0.6652 | S=0.8120\n",
            "E34: Loss=0.2098 | F1-bin=0.9688 | F1-mac=0.6694 | S=0.8191\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import json\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random\n",
        "\n",
        "# ==============================\n",
        "# Feature Engineering Functions\n",
        "# ==============================\n",
        "\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "    \"\"\"Remove gravity component from accelerometer data\"\"\"\n",
        "    if isinstance(acc_data, pd.DataFrame):\n",
        "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    else:\n",
        "        acc_values = acc_data\n",
        "\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = acc_values.shape[0]\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "    gravity_world = np.array([0, 0, 9.802])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "\n",
        "    return linear_accel\n",
        "\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
        "    \"\"\"Calculate angular velocity from quaternion derivatives\"\"\"\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_vel = np.zeros((num_samples, 3))\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q_t = quat_values[i]\n",
        "        q_t_plus_dt = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    return angular_vel\n",
        "\n",
        "def calculate_angular_distance(rot_data):\n",
        "    \"\"\"Calculate angular distance between successive quaternions\"\"\"\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_dist = np.zeros(num_samples)\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q1 = quat_values[i]\n",
        "        q2 = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
        "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
        "            angular_dist[i] = 0\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            r1 = R.from_quat(q1)\n",
        "            r2 = R.from_quat(q2)\n",
        "            relative_rotation = r1.inv() * r2\n",
        "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
        "            angular_dist[i] = angle\n",
        "        except ValueError:\n",
        "            angular_dist[i] = 0\n",
        "\n",
        "    return angular_dist\n",
        "\n",
        "def apply_imu_feature_engineering(df):\n",
        "    \"\"\"Apply IMU feature engineering to the DataFrame\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Replace NaN values with 0 in rotation columns\n",
        "    rotation_cols = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n",
        "    df[rotation_cols] = df[rotation_cols].fillna(0)\n",
        "\n",
        "    # Base features\n",
        "    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
        "    df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
        "\n",
        "    # Derivatives\n",
        "    df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
        "    df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
        "\n",
        "    # Gravity-removed acceleration\n",
        "    linear_accel_list = []\n",
        "    for seq_id, group in df.groupby('sequence_id'):\n",
        "        acc_data = group[['acc_x', 'acc_y', 'acc_z']]\n",
        "        rot_data = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "        linear_accel = remove_gravity_from_acc(acc_data, rot_data)\n",
        "        linear_accel_list.append(\n",
        "            pd.DataFrame(linear_accel, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index)\n",
        "        )\n",
        "    df = pd.concat([df, pd.concat(linear_accel_list)], axis=1)\n",
        "\n",
        "    # Linear acceleration features\n",
        "    df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
        "    df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
        "\n",
        "    # Angular velocity\n",
        "    angular_vel_list = []\n",
        "    for seq_id, group in df.groupby('sequence_id'):\n",
        "        rot_data = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "        angular_vel = calculate_angular_velocity_from_quat(rot_data)\n",
        "        angular_vel_list.append(\n",
        "            pd.DataFrame(angular_vel, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index)\n",
        "        )\n",
        "    df = pd.concat([df, pd.concat(angular_vel_list)], axis=1)\n",
        "\n",
        "    # Angular distance\n",
        "    angular_dist_list = []\n",
        "    for seq_id, group in df.groupby('sequence_id'):\n",
        "        rot_data = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "        angular_dist = calculate_angular_distance(rot_data)\n",
        "        angular_dist_list.append(\n",
        "            pd.DataFrame(angular_dist, columns=['angular_distance'], index=group.index)\n",
        "        )\n",
        "    df = pd.concat([df, pd.concat(angular_dist_list)], axis=1)\n",
        "\n",
        "    # Define all features\n",
        "    imu_cols = ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w'] + [\n",
        "        # Engineered features\n",
        "        'acc_mag', 'rot_angle',\n",
        "        'acc_mag_jerk', 'rot_angle_vel',\n",
        "        'linear_acc_x', 'linear_acc_y', 'linear_acc_z',\n",
        "        'linear_acc_mag', 'linear_acc_mag_jerk',\n",
        "        'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
        "        'angular_distance'\n",
        "    ]\n",
        "\n",
        "    return df, imu_cols\n",
        "\n",
        "# ==============================\n",
        "# Model Architecture Components\n",
        "# ==============================\n",
        "\n",
        "class ResidualBlock1D(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class FPN1D(nn.Module):\n",
        "    def __init__(self, in_channels=256, out_channels=256):\n",
        "        super().__init__()\n",
        "        self.lateral_conv0 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "        self.lateral_conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "        self.lateral_conv2 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        self.smooth_conv0 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.smooth_conv1 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.smooth_conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, c0):\n",
        "        c1 = F.max_pool1d(c0, kernel_size=2, stride=2, ceil_mode=True)\n",
        "        c2 = F.max_pool1d(c1, kernel_size=2, stride=2, ceil_mode=True)\n",
        "\n",
        "        p0 = self.lateral_conv0(c0)\n",
        "        p1 = self.lateral_conv1(c1)\n",
        "        p2 = self.lateral_conv2(c2)\n",
        "\n",
        "        p2_up = F.interpolate(p2, size=p1.size(2), mode='linear', align_corners=True)\n",
        "        p1_combined = p1 + p2_up\n",
        "        p1_smoothed = self.smooth_conv1(p1_combined)\n",
        "\n",
        "        p1_up = F.interpolate(p1_smoothed, size=p0.size(2), mode='linear', align_corners=True)\n",
        "        p0_combined = p0 + p1_up\n",
        "        p0_smoothed = self.smooth_conv0(p0_combined)\n",
        "\n",
        "        return p0_smoothed\n",
        "\n",
        "class SensorBranch(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # 1Ã—1 projection\n",
        "        self.input_proj = nn.Conv1d(input_dim, 256, kernel_size=1)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_cnn = nn.Sequential(\n",
        "            ResidualBlock1D(256),\n",
        "            ResidualBlock1D(256),\n",
        "            ResidualBlock1D(256),\n",
        "        )\n",
        "\n",
        "        # Feature Pyramid Network\n",
        "        self.fpn = FPN1D(256, 256)\n",
        "\n",
        "        # GRU layers\n",
        "        self.gru1 = nn.GRU(256, hidden_dim, num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.gru2 = nn.GRU(hidden_dim*2, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.gru3 = nn.GRU(hidden_dim*2, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Skip connection\n",
        "        self.skip_proj = nn.Linear(256, hidden_dim*2)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name and 'gru' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name and 'gru' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif isinstance(param, nn.Linear):\n",
        "                nn.init.xavier_uniform_(param)\n",
        "                if param.bias is not None:\n",
        "                    nn.init.zeros_(param.bias)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Input transformation\n",
        "        x = x.transpose(1, 2)  # (B, C, T)\n",
        "        x = self.input_proj(x)\n",
        "        x = self.res_cnn(x)\n",
        "        x = self.fpn(x)\n",
        "        x = x.transpose(1, 2)  # (B, T, C)\n",
        "\n",
        "        # GRU processing\n",
        "        packed1 = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out1, _ = self.gru1(packed1)\n",
        "        pad1, _ = pad_packed_sequence(out1, batch_first=True)\n",
        "        skip1 = self.skip_proj(x)\n",
        "        prev = pad1 + skip1\n",
        "\n",
        "        packed2 = pack_padded_sequence(prev, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out2, _ = self.gru2(packed2)\n",
        "        pad2, _ = pad_packed_sequence(out2, batch_first=True)\n",
        "        prev = pad2 + prev\n",
        "\n",
        "        packed3 = pack_padded_sequence(prev, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out3, _ = self.gru3(packed3)\n",
        "        pad3, _ = pad_packed_sequence(out3, batch_first=True)\n",
        "        prev = pad3 + prev\n",
        "\n",
        "        # Last timestep\n",
        "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, prev.size(2))\n",
        "        last = prev.gather(1, idx).squeeze(1)\n",
        "\n",
        "        return last\n",
        "\n",
        "class MultiSensorClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 imu_input_dim=20,  # Updated for engineered features\n",
        "                 thm_input_dim=5,\n",
        "                 tof_input_dim=64,\n",
        "                 hidden_dim=256,\n",
        "                 num_heads=8,\n",
        "                 ffn_hidden=256,\n",
        "                 num_classes=18,\n",
        "                 p_branch_mask=0.1,\n",
        "                 p_feat_dropout=0.2):\n",
        "        super().__init__()\n",
        "        # Sensor branches\n",
        "        self.imu_branch = SensorBranch(imu_input_dim, hidden_dim)\n",
        "        self.thm_branch = SensorBranch(thm_input_dim, hidden_dim)\n",
        "        self.tof_branches = nn.ModuleList([\n",
        "            SensorBranch(tof_input_dim, hidden_dim) for _ in range(5)\n",
        "        ])\n",
        "\n",
        "        # Feature fusion\n",
        "        self.pre_norm = nn.LayerNorm(hidden_dim*2)\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.ModuleDict({\n",
        "                'attention': nn.MultiheadAttention(\n",
        "                    embed_dim=hidden_dim*2,\n",
        "                    num_heads=num_heads,\n",
        "                    batch_first=True\n",
        "                ),\n",
        "                'norm': nn.LayerNorm(hidden_dim*2)\n",
        "            }) for _ in range(3)\n",
        "        ])\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2, ffn_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ffn_hidden, hidden_dim*2)\n",
        "        )\n",
        "        self.post_norm = nn.LayerNorm(hidden_dim*2)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(hidden_dim*2, num_classes)\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "        # Regularization\n",
        "        self.p_branch_mask = p_branch_mask\n",
        "        self.feat_dropout = nn.Dropout(p_feat_dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                imu_seq, imu_len,\n",
        "                thm_seq, thm_len,\n",
        "                tof_inputs, tof_len,\n",
        "                tof_attention_masks,\n",
        "                return_features=False):\n",
        "        # Process IMU\n",
        "        imu_feat = self.imu_branch(imu_seq, imu_len)\n",
        "\n",
        "        # Process THM\n",
        "        thm_feat = self.thm_branch(thm_seq, thm_len)\n",
        "\n",
        "        # Process ToF sensors\n",
        "        tof_feats = []\n",
        "        for i, branch in enumerate(self.tof_branches):\n",
        "            masked_tof = tof_inputs[i].clone()\n",
        "            masked_tof[~tof_attention_masks[i]] = 0\n",
        "            tof_feat = branch(masked_tof, tof_len)\n",
        "            tof_feats.append(tof_feat)\n",
        "\n",
        "        # Combine features\n",
        "        tokens = [imu_feat, thm_feat] + tof_feats\n",
        "        x = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # Branch masking\n",
        "        attention_mask = None\n",
        "        if self.training and self.p_branch_mask > 0:\n",
        "            batch_mask = torch.bernoulli(\n",
        "                torch.full((len(tokens),), 1 - self.p_branch_mask, device=x.device)\n",
        "            )\n",
        "            attention_mask = (~batch_mask.bool()).expand(x.size(0), -1)\n",
        "\n",
        "        # Feature fusion\n",
        "        x = self.pre_norm(x)\n",
        "        x = self.feat_dropout(x)\n",
        "\n",
        "        for attn_layer in self.attention_layers:\n",
        "            attn_out, _ = attn_layer['attention'](\n",
        "                x, x, x,\n",
        "                key_padding_mask=attention_mask\n",
        "            )\n",
        "            x = x + attn_out\n",
        "            x = attn_layer['norm'](x)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            mask_expanded = (~attention_mask).float().unsqueeze(-1)\n",
        "            fused = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)\n",
        "        else:\n",
        "            fused = x.mean(dim=1)\n",
        "\n",
        "        out = self.ffn(fused)\n",
        "        out = self.post_norm(out)\n",
        "        logits = self.classifier(out)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, out\n",
        "        return logits\n",
        "\n",
        "# ==============================\n",
        "# Data Loading & Augmentation\n",
        "# ==============================\n",
        "\n",
        "class MultiSensorDataset(Dataset):\n",
        "    def __init__(self, df, gesture2idx, augment=False, imu_cols=None):\n",
        "        self.samples = []\n",
        "        self.gesture2idx = gesture2idx\n",
        "        self.augment = augment\n",
        "        self.imu_cols = imu_cols or ['acc_x','acc_y','acc_z','rot_w','rot_x','rot_y','rot_z']\n",
        "\n",
        "        # Group by sequence_id\n",
        "        grouped = df.groupby('sequence_id')\n",
        "        self.sequence_ids = list(grouped.groups.keys())\n",
        "\n",
        "        # Store all sequences\n",
        "        for sid, group in grouped:\n",
        "            group = group.sort_values('sequence_counter')\n",
        "\n",
        "            # IMU data - use engineered features\n",
        "            imu = np.nan_to_num(group[self.imu_cols].values.astype(np.float32))\n",
        "\n",
        "            # THM data\n",
        "            thm_arr = np.nan_to_num(group[[f'thm_{i}' for i in range(1,6)]].values.astype(np.float32))\n",
        "\n",
        "            # ToF data\n",
        "            tofs = []\n",
        "            tof_masks = []\n",
        "            for i in range(1,6):\n",
        "                arr = group[[f'tof_{i}_v{j}' for j in range(64)]].values.astype(np.float32)\n",
        "                is_valid = ~(np.isnan(arr).all(axis=1) | (arr == -1).all(axis=1))\n",
        "                tof_masks.append(is_valid)\n",
        "                arr[arr == -1] = 512\n",
        "                arr = np.nan_to_num(arr, nan=1000)\n",
        "                tofs.append(arr)\n",
        "\n",
        "            # Gesture label\n",
        "            gesture = group['gesture'].iloc[0]\n",
        "            label = self.gesture2idx[gesture]\n",
        "            subject = group['subject'].iloc[0]\n",
        "\n",
        "            self.samples.append({\n",
        "                'imu': imu,\n",
        "                'thm': thm_arr,\n",
        "                'tofs': tofs,\n",
        "                'tof_masks': tof_masks,\n",
        "                'label': label,\n",
        "                'gesture': gesture,\n",
        "                'sequence_id': sid,\n",
        "                'subject': subject\n",
        "            })\n",
        "\n",
        "        # Build gesture map for augmentation\n",
        "        if augment:\n",
        "            self.gesture_map = {}\n",
        "            for sample in self.samples:\n",
        "                gesture = sample['gesture']\n",
        "                if gesture not in self.gesture_map:\n",
        "                    self.gesture_map[gesture] = []\n",
        "                self.gesture_map[gesture].append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples) * 3 if self.augment else len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        n = len(self.samples)\n",
        "\n",
        "        # Determine base sample index and sample type\n",
        "        base_idx = idx % n\n",
        "        sample_type = idx // n\n",
        "\n",
        "        # Original sample (first copy)\n",
        "        if sample_type == 0:\n",
        "            sample = self.samples[base_idx]\n",
        "            return (\n",
        "                sample['imu'],\n",
        "                sample['thm'],\n",
        "                sample['tofs'],\n",
        "                sample['tof_masks'],\n",
        "                sample['label']\n",
        "            )\n",
        "\n",
        "        # Mixup samples (remaining copies)\n",
        "        else:\n",
        "            base_sample = self.samples[base_idx]\n",
        "            gesture = base_sample['gesture']\n",
        "            candidates = self.gesture_map.get(gesture, [])\n",
        "\n",
        "            # Fallback to original if no suitable partner\n",
        "            if len(candidates) < 2:\n",
        "                return (\n",
        "                    base_sample['imu'],\n",
        "                    base_sample['thm'],\n",
        "                    base_sample['tofs'],\n",
        "                    base_sample['tof_masks'],\n",
        "                    base_sample['label']\n",
        "                )\n",
        "\n",
        "            # Select different sequence\n",
        "            partner_sample = base_sample\n",
        "            while partner_sample['sequence_id'] == base_sample['sequence_id']:\n",
        "                partner_sample = random.choice(candidates)\n",
        "\n",
        "            # Mixup parameters\n",
        "            lam = np.random.beta(0.4, 0.4)\n",
        "\n",
        "            # Mix IMU data\n",
        "            min_imu = min(base_sample['imu'].shape[0], partner_sample['imu'].shape[0])\n",
        "            mixed_imu = lam * base_sample['imu'][:min_imu] + (1 - lam) * partner_sample['imu'][:min_imu]\n",
        "\n",
        "            # Mix THM data\n",
        "            min_thm = min(base_sample['thm'].shape[0], partner_sample['thm'].shape[0])\n",
        "            mixed_thm = lam * base_sample['thm'][:min_thm] + (1 - lam) * partner_sample['thm'][:min_thm]\n",
        "\n",
        "            # Mix ToF data\n",
        "            mixed_tofs = []\n",
        "            for i in range(5):\n",
        "                min_tof = min(base_sample['tofs'][i].shape[0], partner_sample['tofs'][i].shape[0])\n",
        "                mixed_tof = lam * base_sample['tofs'][i][:min_tof] + (1 - lam) * partner_sample['tofs'][i][:min_tof]\n",
        "                mixed_tofs.append(mixed_tof)\n",
        "\n",
        "            # Mix masks (OR operation)\n",
        "            mixed_masks = []\n",
        "            for i in range(5):\n",
        "                min_len = min(base_sample['tof_masks'][i].shape[0], partner_sample['tof_masks'][i].shape[0])\n",
        "                mask1 = base_sample['tof_masks'][i][:min_len]\n",
        "                mask2 = partner_sample['tof_masks'][i][:min_len]\n",
        "                mixed_masks.append(mask1 | mask2)\n",
        "\n",
        "            return (\n",
        "                mixed_imu,\n",
        "                mixed_thm,\n",
        "                mixed_tofs,\n",
        "                mixed_masks,\n",
        "                base_sample['label']  # Same label\n",
        "            )\n",
        "\n",
        "# ==============================\n",
        "# Collate Function (No Normalization)\n",
        "# ==============================\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imu_seqs, thm_seqs, tof_seqs_list, tof_masks_list, labels = zip(*batch)\n",
        "\n",
        "    # Sequence lengths\n",
        "    imu_len = torch.tensor([s.shape[0] for s in imu_seqs], dtype=torch.long)\n",
        "    thm_len = torch.tensor([s.shape[0] for s in thm_seqs], dtype=torch.long)\n",
        "    tof_len = torch.tensor([tofs[0].shape[0] for tofs in tof_seqs_list], dtype=torch.long)\n",
        "\n",
        "    # Pad IMU\n",
        "    imu_padded = nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(s, dtype=torch.float32) for s in imu_seqs],\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    # Pad THM\n",
        "    thm_padded = nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(s, dtype=torch.float32) for s in thm_seqs],\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    # Pad ToF sensors\n",
        "    tof_padded = []\n",
        "    for i in range(5):\n",
        "        sensor_seqs = [s[i] for s in tof_seqs_list]\n",
        "        padded = nn.utils.rnn.pad_sequence(\n",
        "            [torch.tensor(s, dtype=torch.float32) for s in sensor_seqs],\n",
        "            batch_first=True\n",
        "        )\n",
        "        tof_padded.append(padded)\n",
        "\n",
        "    # Pad ToF masks\n",
        "    tof_masks_padded = []\n",
        "    max_t_tof = max(tof_len)\n",
        "    for i in range(5):\n",
        "        sensor_masks = []\n",
        "        for mask in tof_masks_list:\n",
        "            m = mask[i]\n",
        "            pad = max_t_tof - m.shape[0]\n",
        "            padded_mask = torch.tensor(\n",
        "                np.pad(m, (0, pad), constant_values=False),\n",
        "                dtype=torch.bool\n",
        "            )\n",
        "            sensor_masks.append(padded_mask)\n",
        "        tof_masks_padded.append(torch.stack(sensor_masks, dim=0))\n",
        "\n",
        "    # Labels\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return (\n",
        "        imu_padded,\n",
        "        imu_len,\n",
        "        thm_padded,\n",
        "        thm_len,\n",
        "        tof_padded,\n",
        "        tof_len,\n",
        "        tof_masks_padded,\n",
        "        labels\n",
        "    )\n",
        "\n",
        "# ==============================\n",
        "# Training & Evaluation\n",
        "# ==============================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in loader:\n",
        "        imu, imu_len, thm, thm_len, tof_inputs, tof_len, tof_masks, labels = batch\n",
        "\n",
        "        # Move to device\n",
        "        imu = imu.to(device)\n",
        "        thm = thm.to(device)\n",
        "        tof_inputs = [t.to(device) for t in tof_inputs]\n",
        "        tof_masks = [m.to(device) for m in tof_masks]\n",
        "        labels = labels.to(device)\n",
        "        imu_len = imu_len.to(device)\n",
        "        thm_len = thm_len.to(device)\n",
        "        tof_len = tof_len.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imu, imu_len, thm, thm_len, tof_inputs, tof_len, tof_masks)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * imu.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader, device, non_target_idxs):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            imu, imu_len, thm, thm_len, tof_inputs, tof_len, tof_masks, labels = batch\n",
        "\n",
        "            # Move to device\n",
        "            imu = imu.to(device)\n",
        "            thm = thm.to(device)\n",
        "            tof_inputs = [t.to(device) for t in tof_inputs]\n",
        "            tof_masks = [m.to(device) for m in tof_masks]\n",
        "            imu_len = imu_len.to(device)\n",
        "            thm_len = thm_len.to(device)\n",
        "            tof_len = tof_len.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(imu, imu_len, thm, thm_len, tof_inputs, tof_len, tof_masks)\n",
        "            p = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "            preds.append(p)\n",
        "            trues.append(labels.numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "\n",
        "    # Binary F1 (target vs non-target)\n",
        "    true_bin = ~np.isin(trues, non_target_idxs)\n",
        "    pred_bin = ~np.isin(preds, non_target_idxs)\n",
        "    f1_binary = f1_score(true_bin, pred_bin)\n",
        "\n",
        "    # Macro F1\n",
        "    f1_macro = f1_score(trues, preds, average='macro')\n",
        "\n",
        "    # Combined score\n",
        "    score = (f1_binary + f1_macro) / 2\n",
        "\n",
        "    return f1_binary, f1_macro, score\n",
        "\n",
        "# ==============================\n",
        "# Main Training Pipeline\n",
        "# ==============================\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_epochs = 50\n",
        "    batch_size = 64\n",
        "    val_batch_size = 128\n",
        "    hidden_dim = 256\n",
        "    lr = 1e-4\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('/content/drive/MyDrive/cmi-detect-behavior-with-sensor-data/train.csv')\n",
        "\n",
        "    # Create gesture mapping\n",
        "    gestures = sorted(df['gesture'].unique())\n",
        "    gesture2idx = {g: i for i, g in enumerate(gestures)}\n",
        "\n",
        "    # Define non-target gestures\n",
        "    non_target_list = [\n",
        "        'Drink from bottle/cup','Glasses on/off','Pull air toward your face',\n",
        "        'Pinch knee/leg skin','Scratch knee/leg skin','Write name on leg',\n",
        "        'Text on phone','Feel around in tray and pull out an object',\n",
        "        'Write name in air','Wave hello'\n",
        "    ]\n",
        "    non_target_idxs = [gesture2idx[g] for g in non_target_list if g in gesture2idx]\n",
        "\n",
        "    # Apply IMU feature engineering to the entire dataset\n",
        "    print(\"Applying IMU feature engineering...\")\n",
        "    df, imu_cols = apply_imu_feature_engineering(df)\n",
        "    print(f\"IMU features expanded to {len(imu_cols)} dimensions\")\n",
        "\n",
        "    # # Create sequence info for stratified grouping\n",
        "    seq_info = df.groupby('sequence_id').agg(\n",
        "        gesture=('gesture', 'first'),\n",
        "        subject=('subject', 'first')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Cross-validation setup\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, vl_idx) in enumerate(\n",
        "        splitter.split(np.zeros(len(seq_info)), seq_info.gesture, groups=seq_info.subject),\n",
        "        start=1\n",
        "    ):\n",
        "        print(f'\\n=== Fold {fold}/5 ===')\n",
        "\n",
        "        # Split data\n",
        "        train_ids = seq_info.sequence_id.iloc[tr_idx]\n",
        "        val_ids = seq_info.sequence_id.iloc[vl_idx]\n",
        "        train_df = df[df.sequence_id.isin(train_ids)].copy()\n",
        "        val_df = df[df.sequence_id.isin(val_ids)].copy()\n",
        "\n",
        "        # Build datasets\n",
        "        train_ds = MultiSensorDataset(train_df, gesture2idx, augment=True, imu_cols=imu_cols)\n",
        "        val_ds = MultiSensorDataset(val_df, gesture2idx, augment=False, imu_cols=imu_cols)\n",
        "\n",
        "        print(f\"Training samples: {len(train_ds)}\")\n",
        "        print(f\"Validation samples: {len(val_ds)}\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_ds, batch_size=batch_size, shuffle=True,\n",
        "            collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_ds, batch_size=val_batch_size, shuffle=False,\n",
        "            collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = MultiSensorClassifier(\n",
        "            imu_input_dim=len(imu_cols),  # Updated for engineered features\n",
        "            thm_input_dim=5,\n",
        "            tof_input_dim=64,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_classes=len(gesture2idx)\n",
        "        ).to(device)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        # Training loop\n",
        "        best_score = 0.0\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "            f1b, f1m, score = evaluate(model, val_loader, device, non_target_idxs)\n",
        "\n",
        "            print(f'E{epoch}: Loss={loss:.4f} | F1-bin={f1b:.4f} | F1-mac={f1m:.4f} | S={score:.4f}')\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                torch.save(model.state_dict(), f'/content/drive/MyDrive/cmi-detect-behavior-with-sensor-data/best_model_fold{fold}.pth')\n",
        "                print(f'Saved new best model with score: {score:.4f}')\n",
        "\n",
        "        print(f'Best score fold {fold}: {best_score:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}